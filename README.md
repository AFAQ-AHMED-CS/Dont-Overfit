# Dont-Overfit
This is an attempt at the competition hosted on [Kaggle](https://www.kaggle.com/c/dont-overfit-ii).The accuracy achieved on the public leaderboard is 0.847.The solution involves using simple stacking of a variety classifiers such as Random Forest Classifier,XGBoost Classifier , Support Vector Machine(RBF Kernel) and Logistic Regression to classify the input as 1 or 0.

The interesting thing about the competition is as the description put it

*It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples... without overfitting.*

It's truly a fun competition to gets your hands dirty.

## Note
I hold no rights to the data its property of kaggle.

.
